nohup: 忽略输入
/home/llq/.conda/envs/torch17_cuda110/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/llq/.conda/envs/torch17_cuda110/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
epoch:1
train: p:0.010998398291510945, r:0.042934556065027094, f:0.01751105066303978
dev: p:0.0, r:0.0, f:0.0
test: p:0.0, r:0.0, f:0.0
epoch:2
train: p:0.7189952904238619, r:0.1909128803668195, f:0.30171277997364954
dev: p:0.6879432624113475, r:0.5878787878787879, f:0.6339869281045752
test: p:0.6869158878504673, r:0.4681528662420382, f:0.5568181818181818
epoch:3
train: p:0.8317107093184979, r:0.49854105877448934, f:0.6234037008079228
dev: p:0.7945205479452054, r:0.703030303030303, f:0.7459807073954984
test: p:0.7644787644787645, r:0.6305732484076433, f:0.6910994764397906
epoch:4
train: p:0.898588604286461, r:0.716548561900792, f:0.7973098330241186
dev: p:0.7733333333333333, r:0.703030303030303, f:0.7365079365079364
test: p:0.757679180887372, r:0.7070063694267515, f:0.7314662273476112
epoch:5
train: p:0.9540284360189574, r:0.8390996248436848, f:0.8928809048569526
dev: p:0.7834394904458599, r:0.7454545454545455, f:0.7639751552795031
test: p:0.8044280442804428, r:0.6942675159235668, f:0.7452991452991453
epoch:6
train: p:0.974706413730804, r:0.8995414756148395, f:0.9356167353132453
dev: p:0.7738095238095238, r:0.7878787878787878, f:0.7807807807807807
test: p:0.7830508474576271, r:0.7356687898089171, f:0.7586206896551724
save best
epoch:7
train: p:0.9854175872735307, r:0.9295539808253439, f:0.9566709566709567
dev: p:0.8050314465408805, r:0.7757575757575758, f:0.7901234567901235
test: p:0.7922535211267606, r:0.7165605095541401, f:0.7525083612040135
save best
epoch:8
train: p:0.9925861317051897, r:0.948728636932055, f:0.9701619778346121
dev: p:0.7784431137724551, r:0.7878787878787878, f:0.783132530120482
test: p:0.7737704918032787, r:0.7515923566878981, f:0.7625201938610664
epoch:9
train: p:0.9939810834049871, r:0.9637348895373072, f:0.9786243386243387
dev: p:0.7797619047619048, r:0.793939393939394, f:0.7867867867867869
test: p:0.7637540453074434, r:0.7515923566878981, f:0.7576243980738363
epoch:10
train: p:0.9957464908549554, r:0.9758232596915382, f:0.9856842105263157
dev: p:0.7777777777777778, r:0.7636363636363637, f:0.7706422018348624
test: p:0.7891156462585034, r:0.7388535031847133, f:0.763157894736842
best epoch: 7, dev f1:0.7901234567901235, test p:0.7922535211267606, r:0.7165605095541401, f1:0.7525083612040135, 
/home/llq/.conda/envs/torch17_cuda110/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/llq/.conda/envs/torch17_cuda110/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
epoch:1
train: p:0.009071546895953301, r:0.047936640266777825, f:0.015256036083841868
dev: p:0.0, r:0.0, f:0.0
test: p:0.0, r:0.0, f:0.0
epoch:2
train: p:0.7288135593220338, r:0.19716548561900793, f:0.31036745406824146
dev: p:0.7064220183486238, r:0.4666666666666667, f:0.5620437956204378
test: p:0.7037037037037037, r:0.42356687898089174, f:0.5288270377733598
epoch:3
train: p:0.8298162014976175, r:0.5081283868278449, f:0.6302998965873836
dev: p:0.7966101694915254, r:0.5696969696969697, f:0.6643109540636043
test: p:0.8494623655913979, r:0.5031847133757962, f:0.632
epoch:4
train: p:0.9120879120879121, r:0.7265527303042935, f:0.8088167053364268
dev: p:0.7682119205298014, r:0.703030303030303, f:0.7341772151898734
test: p:0.7777777777777778, r:0.6464968152866242, f:0.706086956521739
epoch:5
train: p:0.9524036173250833, r:0.8340975406419341, f:0.8893333333333332
dev: p:0.7730061349693251, r:0.7636363636363637, f:0.7682926829268293
test: p:0.7641196013289037, r:0.732484076433121, f:0.7479674796747968
epoch:6
train: p:0.9746031746031746, r:0.8957899124635265, f:0.9335360556038228
dev: p:0.7484662576687117, r:0.7393939393939394, f:0.7439024390243902
test: p:0.7789115646258503, r:0.7292993630573248, f:0.7532894736842104
save best
epoch:7
train: p:0.9854433171592413, r:0.9312213422259275, f:0.9575653664809259
dev: p:0.7575757575757576, r:0.7575757575757576, f:0.7575757575757576
test: p:0.7781569965870307, r:0.7261146496815286, f:0.7512355848434926
save best
epoch:8
train: p:0.9938997821350762, r:0.9508128386827845, f:0.9718789944610141
dev: p:0.7674418604651163, r:0.8, f:0.7833827893175075
test: p:0.7777777777777778, r:0.7356687898089171, f:0.7561374795417348
save best
epoch:9
train: p:0.9961356805495921, r:0.9670696123384743, f:0.9813874788494078
dev: p:0.7647058823529411, r:0.7878787878787878, f:0.7761194029850745
test: p:0.7837837837837838, r:0.7388535031847133, f:0.7606557377049181
epoch:10
train: p:0.9982956966340009, r:0.9766569403918299, f:0.9873577749683945
dev: p:0.7738095238095238, r:0.7878787878787878, f:0.7807807807807807
test: p:0.7903780068728522, r:0.732484076433121, f:0.7603305785123967
best epoch: 8, dev f1:0.7833827893175075, test p:0.7777777777777778, r:0.7356687898089171, f1:0.7561374795417348, 
/home/llq/.conda/envs/torch17_cuda110/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/llq/.conda/envs/torch17_cuda110/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
epoch:1
train: p:0.014482914686580674, r:0.08003334722801167, f:0.02452733776188043
dev: p:0.0, r:0.0, f:0.0
test: p:0.0, r:0.0, f:0.0
epoch:2
train: p:0.7564356435643564, r:0.15923301375573157, f:0.2630853994490358
dev: p:0.7102803738317757, r:0.46060606060606063, f:0.5588235294117647
test: p:0.6855670103092784, r:0.42356687898089174, f:0.5236220472440946
epoch:3
train: p:0.8084526244035446, r:0.4943726552730304, f:0.6135540610450076
dev: p:0.7786259541984732, r:0.6181818181818182, f:0.6891891891891891
test: p:0.8436018957345972, r:0.5668789808917197, f:0.678095238095238
epoch:4
train: p:0.9039665970772442, r:0.7219674864526886, f:0.8027809965237543
dev: p:0.7808219178082192, r:0.6909090909090909, f:0.7331189710610932
test: p:0.7944664031620553, r:0.6401273885350318, f:0.7089947089947091
epoch:5
train: p:0.9484978540772532, r:0.8290954564401835, f:0.8847864768683273
dev: p:0.7590361445783133, r:0.7636363636363637, f:0.7613293051359517
test: p:0.7727272727272727, r:0.7038216560509554, f:0.7366666666666666
epoch:6
train: p:0.9684115523465704, r:0.8945393914130888, f:0.9300108342361864
dev: p:0.7875, r:0.7636363636363637, f:0.7753846153846152
test: p:0.7837837837837838, r:0.7388535031847133, f:0.7606557377049181
save best
epoch:7
train: p:0.9845814977973568, r:0.9316381825760733, f:0.9573784536303276
dev: p:0.7848101265822784, r:0.7515151515151515, f:0.7678018575851393
test: p:0.7916666666666666, r:0.7261146496815286, f:0.7574750830564784
epoch:8
train: p:0.9917713295799048, r:0.9545644018340975, f:0.9728122344944776
dev: p:0.7607361963190185, r:0.7515151515151515, f:0.7560975609756098
test: p:0.7965517241379311, r:0.7356687898089171, f:0.7649006622516555
epoch:9
train: p:0.9961423060437206, r:0.968736973739058, f:0.9822485207100592
dev: p:0.7884615384615384, r:0.7454545454545455, f:0.766355140186916
test: p:0.8043478260869565, r:0.7070063694267515, f:0.7525423728813558
epoch:10
train: p:0.9974446337308348, r:0.976240100041684, f:0.9867284600800507
dev: p:0.78125, r:0.7575757575757576, f:0.7692307692307692
test: p:0.7950530035335689, r:0.7165605095541401, f:0.7537688442211056
best epoch: 6, dev f1:0.7753846153846152, test p:0.7837837837837838, r:0.7388535031847133, f1:0.7606557377049181, 
epoch:1
train: p:0.013108839446782922, r:0.04543559816590246, f:0.020347209258913573
dev: p:0.7727272727272727, r:0.10303030303030303, f:0.18181818181818182
test: p:0.8055555555555556, r:0.09235668789808917, f:0.1657142857142857
epoch:2
train: p:0.7777777777777778, r:0.21008753647353065, f:0.33081719724319003
dev: p:0.75, r:0.6, f:0.6666666666666665
test: p:0.7412280701754386, r:0.5382165605095541, f:0.6236162361623616
epoch:3
train: p:0.8533246414602347, r:0.5456440183409754, f:0.6656496313246886
dev: p:0.7973856209150327, r:0.7393939393939394, f:0.7672955974842767
test: p:0.7653429602888087, r:0.6751592356687898, f:0.7174280879864635
epoch:4
train: p:0.9122632103688934, r:0.7628178407669862, f:0.8308740068104427
dev: p:0.7986111111111112, r:0.696969696969697, f:0.7443365695792881
test: p:0.8082706766917294, r:0.6847133757961783, f:0.7413793103448276
epoch:5
train: p:0.9536152796725784, r:0.8741142142559399, f:0.9121357111787735
dev: p:0.7607361963190185, r:0.7515151515151515, f:0.7560975609756098
test: p:0.7679180887372014, r:0.7165605095541401, f:0.7413509060955519
epoch:6
train: p:0.9782318969346957, r:0.9178824510212589, f:0.9470967741935483
dev: p:0.7513513513513513, r:0.8424242424242424, f:0.7942857142857143
test: p:0.7325227963525835, r:0.767515923566879, f:0.749611197511664
save best
epoch:7
train: p:0.9839618552232337, r:0.9462275948311797, f:0.9647258818529537
dev: p:0.7692307692307693, r:0.7878787878787878, f:0.778443113772455
test: p:0.7796052631578947, r:0.7547770700636943, f:0.766990291262136
epoch:8
train: p:0.9931359931359931, r:0.9649854105877449, f:0.9788583509513742
dev: p:0.7738095238095238, r:0.7878787878787878, f:0.7807807807807807
test: p:0.7580645161290323, r:0.7484076433121019, f:0.7532051282051282
epoch:9
train: p:0.9965971926839643, r:0.9766569403918299, f:0.9865263157894737
dev: p:0.7797619047619048, r:0.793939393939394, f:0.7867867867867869
test: p:0.7796052631578947, r:0.7547770700636943, f:0.766990291262136
epoch:10
train: p:0.9983000424989376, r:0.9791579824927052, f:0.9886363636363635
dev: p:0.764367816091954, r:0.806060606060606, f:0.7846607669616519
test: p:0.7702265372168284, r:0.7579617834394905, f:0.7640449438202247
best epoch: 6, dev f1:0.7942857142857143, test p:0.7325227963525835, r:0.767515923566879, f1:0.749611197511664, 
epoch:1
train: p:0.012412814753516964, r:0.04376823676531888, f:0.019340578375391413
dev: p:0.9411764705882353, r:0.09696969696969697, f:0.17582417582417584
test: p:1.0, r:0.054140127388535034, f:0.10271903323262839
epoch:2
train: p:0.7651403249630724, r:0.21592330137557317, f:0.3368010403120936
dev: p:0.7461538461538462, r:0.5878787878787879, f:0.6576271186440679
test: p:0.7432432432432432, r:0.5254777070063694, f:0.6156716417910447
epoch:3
train: p:0.8433354390397979, r:0.5564818674447687, f:0.6705173279758915
dev: p:0.8088235294117647, r:0.6666666666666666, f:0.7308970099667773
test: p:0.8154506437768241, r:0.6050955414012739, f:0.6946983546617915
epoch:4
train: p:0.9197592778335005, r:0.7644852021675698, f:0.8349647165945823
dev: p:0.7828947368421053, r:0.7212121212121212, f:0.750788643533123
test: p:0.7818181818181819, r:0.6847133757961783, f:0.730050933786078
epoch:5
train: p:0.950984883188273, r:0.8653605669028762, f:0.9061545176778699
dev: p:0.75, r:0.7818181818181819, f:0.7655786350148369
test: p:0.7833333333333333, r:0.7484076433121019, f:0.765472312703583
epoch:6
train: p:0.9764758100310696, r:0.9170487703209671, f:0.9458297506448838
dev: p:0.7636363636363637, r:0.7636363636363637, f:0.7636363636363637
test: p:0.7596153846153846, r:0.7547770700636943, f:0.757188498402556
save best
epoch:7
train: p:0.9873362445414847, r:0.9424760316798666, f:0.9643847302196631
dev: p:0.7701149425287356, r:0.8121212121212121, f:0.7905604719764011
test: p:0.7538940809968847, r:0.7707006369426752, f:0.7622047244094489
save best
epoch:8
train: p:0.990990990990991, r:0.9629012088370155, f:0.9767441860465117
dev: p:0.7514450867052023, r:0.7878787878787878, f:0.7692307692307692
test: p:0.7658227848101266, r:0.7707006369426752, f:0.7682539682539683
epoch:9
train: p:0.9970174691095015, r:0.9754064193413923, f:0.9860935524652339
dev: p:0.7514450867052023, r:0.7878787878787878, f:0.7692307692307692
test: p:0.7665615141955836, r:0.7738853503184714, f:0.7702060221870048
epoch:10
train: p:0.9978777589134126, r:0.9799916631929971, f:0.9888538380651946
dev: p:0.7456647398843931, r:0.7818181818181819, f:0.7633136094674556
test: p:0.765079365079365, r:0.767515923566879, f:0.7662957074721781
best epoch: 7, dev f1:0.7905604719764011, test p:0.7538940809968847, r:0.7707006369426752, f1:0.7622047244094489, 
epoch:1
train: p:0.014500836586726157, r:0.04335139641517299, f:0.02173231637237488
dev: p:0.875, r:0.12727272727272726, f:0.22222222222222224
test: p:0.8181818181818182, r:0.08598726114649681, f:0.15561959654178675
epoch:2
train: p:0.7934131736526946, r:0.22092538557732389, f:0.345614607107923
dev: p:0.7777777777777778, r:0.509090909090909, f:0.6153846153846153
test: p:0.7333333333333333, r:0.4554140127388535, f:0.5618860510805501
epoch:3
train: p:0.8498360655737704, r:0.5402250937890788, f:0.6605504587155963
dev: p:0.7916666666666666, r:0.6909090909090909, f:0.737864077669903
test: p:0.8339622641509434, r:0.7038216560509554, f:0.7633851468048359
epoch:4
train: p:0.9078685258964143, r:0.759899958315965, f:0.8273201724529159
dev: p:0.7973856209150327, r:0.7393939393939394, f:0.7672955974842767
test: p:0.7750865051903114, r:0.7133757961783439, f:0.7429519071310116
epoch:5
train: p:0.9548802946593001, r:0.8645268862025844, f:0.9074600743819734
dev: p:0.7716049382716049, r:0.7575757575757576, f:0.7645259938837919
test: p:0.7760252365930599, r:0.7834394904458599, f:0.779714738510301
epoch:6
train: p:0.9720496894409938, r:0.913297207169654, f:0.9417580055877929
dev: p:0.7735849056603774, r:0.7454545454545455, f:0.7592592592592593
test: p:0.7864077669902912, r:0.7738853503184714, f:0.7800963081861958
save best
epoch:7
train: p:0.9843953185955787, r:0.9466444351813256, f:0.9651508712282194
dev: p:0.7672955974842768, r:0.7393939393939394, f:0.7530864197530864
test: p:0.7554858934169278, r:0.767515923566879, f:0.7614533965244865
epoch:8
train: p:0.9931330472103004, r:0.964568570237599, f:0.9786424191160922
dev: p:0.7672955974842768, r:0.7393939393939394, f:0.7530864197530864
test: p:0.7987012987012987, r:0.7834394904458599, f:0.7909967845659164
epoch:9
train: p:0.9957392415850022, r:0.9741558982909546, f:0.9848293299620734
dev: p:0.7730061349693251, r:0.7636363636363637, f:0.7682926829268293
test: p:0.7725856697819314, r:0.7898089171974523, f:0.7811023622047244
save best
epoch:10
train: p:0.9957752429235319, r:0.9824927052938724, f:0.9890893831305078
dev: p:0.7636363636363637, r:0.7636363636363637, f:0.7636363636363637
test: p:0.7638036809815951, r:0.7929936305732485, f:0.778125
best epoch: 9, dev f1:0.7682926829268293, test p:0.7725856697819314, r:0.7898089171974523, f1:0.7811023622047244, 
Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
epoch:1
train: p:0.013336962438758846, r:0.04085035431429762, f:0.02010875141069047
dev: p:0.8055555555555556, r:0.17575757575757575, f:0.2885572139303483
test: p:0.9285714285714286, r:0.12420382165605096, f:0.2191011235955056
epoch:2
train: p:0.8112338858195212, r:0.3672363484785327, f:0.5055954088952654
dev: p:0.8309859154929577, r:0.7151515151515152, f:0.7687296416938112
test: p:0.8007518796992481, r:0.678343949044586, f:0.7344827586206896
epoch:3
train: p:0.874361593462717, r:0.7136306794497708, f:0.7858618315354602
dev: p:0.7971014492753623, r:0.6666666666666666, f:0.7260726072607261
test: p:0.8392156862745098, r:0.6815286624203821, f:0.7521968365553602
epoch:4
train: p:0.9430291801760075, r:0.8486869528970404, f:0.8933742869679685
dev: p:0.7579617834394905, r:0.7212121212121212, f:0.7391304347826086
test: p:0.8191489361702128, r:0.7356687898089171, f:0.7751677852348993
epoch:5
train: p:0.9724288840262582, r:0.9262192580241767, f:0.9487617421007686
dev: p:0.7848101265822784, r:0.7515151515151515, f:0.7678018575851393
test: p:0.7985347985347986, r:0.6942675159235668, f:0.7427597955706985
epoch:6
train: p:0.9897654584221749, r:0.9674864526886202, f:0.9784991568296796
dev: p:0.7471264367816092, r:0.7878787878787878, f:0.7669616519174042
test: p:0.7734627831715211, r:0.7611464968152867, f:0.7672552166934191
save best
epoch:7
train: p:0.9944938585345193, r:0.9787411421425594, f:0.9865546218487395
dev: p:0.7777777777777778, r:0.7636363636363637, f:0.7706422018348624
test: p:0.8047945205479452, r:0.7484076433121019, f:0.7755775577557755
save best
epoch:8
train: p:0.9974758098443416, r:0.988328470195915, f:0.9928810720268006
dev: p:0.7696969696969697, r:0.7696969696969697, f:0.7696969696969697
test: p:0.7951388888888888, r:0.7292993630573248, f:0.760797342192691
epoch:9
train: p:0.9987384356602187, r:0.9899958315964985, f:0.9943479171027843
dev: p:0.7633136094674556, r:0.7818181818181819, f:0.7724550898203593
test: p:0.7946127946127947, r:0.7515923566878981, f:0.7725040916530278
save best
epoch:10
train: p:0.9995812395309883, r:0.9949979157982493, f:0.9972843116774598
dev: p:0.7633136094674556, r:0.7818181818181819, f:0.7724550898203593
test: p:0.7891156462585034, r:0.7388535031847133, f:0.763157894736842
best epoch: 9, dev f1:0.7724550898203593, test p:0.7946127946127947, r:0.7515923566878981, f1:0.7725040916530278, 
Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
epoch:1
train: p:0.01580135440180587, r:0.04085035431429762, f:0.022788047901406813
dev: p:0.9069767441860465, r:0.23636363636363636, f:0.375
test: p:0.9285714285714286, r:0.16560509554140126, f:0.2810810810810811
epoch:2
train: p:0.8107416879795396, r:0.3964151729887453, f:0.5324748040313549
dev: p:0.823076923076923, r:0.6484848484848484, f:0.7254237288135592
test: p:0.8348214285714286, r:0.5955414012738853, f:0.6951672862453532
epoch:3
train: p:0.88268156424581, r:0.724468528553564, f:0.7957875457875458
dev: p:0.7607361963190185, r:0.7515151515151515, f:0.7560975609756098
test: p:0.7728937728937729, r:0.6719745222929936, f:0.7189097103918227
epoch:4
train: p:0.9441923774954628, r:0.8674447686536056, f:0.904192917662394
dev: p:0.7633136094674556, r:0.7818181818181819, f:0.7724550898203593
test: p:0.7940199335548173, r:0.7611464968152867, f:0.7772357723577237
epoch:5
train: p:0.9759720401922237, r:0.9312213422259275, f:0.9530716723549487
dev: p:0.7415730337078652, r:0.8, f:0.7696793002915452
test: p:0.7841269841269841, r:0.7866242038216561, f:0.7853736089030207
epoch:6
train: p:0.9867634500426985, r:0.9633180491871614, f:0.9748998101666315
dev: p:0.73224043715847, r:0.8121212121212121, f:0.7701149425287357
test: p:0.7877813504823151, r:0.7802547770700637, f:0.7840000000000001
save best
epoch:7
train: p:0.9932346723044397, r:0.9791579824927052, f:0.986146095717884
dev: p:0.7647058823529411, r:0.7878787878787878, f:0.7761194029850745
test: p:0.7902097902097902, r:0.7197452229299363, f:0.7533333333333333
save best
epoch:8
train: p:0.9974747474747475, r:0.9879116298457691, f:0.9926701570680628
dev: p:0.7486033519553073, r:0.8121212121212121, f:0.7790697674418605
test: p:0.7960526315789473, r:0.7707006369426752, f:0.7831715210355987
save best
epoch:9
train: p:0.9991624790619765, r:0.9945810754481034, f:0.9968665134739921
dev: p:0.7541899441340782, r:0.8181818181818182, f:0.7848837209302325
test: p:0.7920792079207921, r:0.7643312101910829, f:0.7779578606158833
save best
epoch:10
train: p:0.9983221476510067, r:0.992080033347228, f:0.9951913025297932
dev: p:0.7570621468926554, r:0.8121212121212121, f:0.7836257309941521
test: p:0.7953795379537953, r:0.767515923566879, f:0.7811993517017829
best epoch: 9, dev f1:0.7848837209302325, test p:0.7920792079207921, r:0.7643312101910829, f1:0.7779578606158833, 
Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext-large were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
epoch:1
train: p:0.012844378257632167, r:0.028761984160066693, f:0.017758332260970275
dev: p:0.684931506849315, r:0.30303030303030304, f:0.42016806722689076
test: p:0.7037037037037037, r:0.24203821656050956, f:0.36018957345971564
epoch:2
train: p:0.797153024911032, r:0.3734889537307211, f:0.5086573942662502
dev: p:0.8425925925925926, r:0.5515151515151515, f:0.6666666666666666
test: p:0.8390243902439024, r:0.5477707006369427, f:0.6628131021194604
epoch:3
train: p:0.8860693940963231, r:0.7132138390996249, f:0.7903002309468822
dev: p:0.7592592592592593, r:0.7454545454545455, f:0.7522935779816514
test: p:0.7847222222222222, r:0.7197452229299363, f:0.7508305647840531
epoch:4
train: p:0.9444699403396053, r:0.8578574406002502, f:0.8990825688073395
dev: p:0.8102189781021898, r:0.6727272727272727, f:0.7350993377483444
test: p:0.8155737704918032, r:0.6337579617834395, f:0.7132616487455197
epoch:5
train: p:0.9724890829694323, r:0.9283034597749062, f:0.9498827042013223
dev: p:0.7664670658682635, r:0.7757575757575758, f:0.7710843373493977
test: p:0.78, r:0.7452229299363057, f:0.762214983713355
epoch:6
train: p:0.9880188275566966, r:0.9624843684868696, f:0.9750844594594595
dev: p:0.7619047619047619, r:0.7757575757575758, f:0.7687687687687688
test: p:0.7965517241379311, r:0.7356687898089171, f:0.7649006622516555
save best
epoch:7
train: p:0.9961685823754789, r:0.9754064193413923, f:0.9856781802864364
dev: p:0.7738095238095238, r:0.7878787878787878, f:0.7807807807807807
test: p:0.7861842105263158, r:0.7611464968152867, f:0.7734627831715212
save best
epoch:8
train: p:0.9966329966329966, r:0.9870779491454773, f:0.9918324607329843
dev: p:0.7668711656441718, r:0.7575757575757576, f:0.7621951219512195
test: p:0.8013698630136986, r:0.7452229299363057, f:0.7722772277227723
epoch:9
train: p:0.997907949790795, r:0.9941642350979575, f:0.9960325746502401
dev: p:0.7619047619047619, r:0.7757575757575758, f:0.7687687687687688
test: p:0.7938144329896907, r:0.7356687898089171, f:0.7636363636363636
epoch:10
train: p:0.9991652754590985, r:0.9979157982492706, f:0.9985401459854014
dev: p:0.7664670658682635, r:0.7757575757575758, f:0.7710843373493977
test: p:0.7972508591065293, r:0.7388535031847133, f:0.7669421487603305
best epoch: 7, dev f1:0.7807807807807807, test p:0.7861842105263158, r:0.7611464968152867, f1:0.7734627831715212, 
